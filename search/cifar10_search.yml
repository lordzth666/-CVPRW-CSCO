# Prepare dataset and network configuration.
task: "cifar10"
data: "./data/cifar-10-batches-py"
lmdb: 1
workers: 4
# Data augmentation
augmentation: "normal-no-cutout"
# Include network config.
prototxt: "target/acenet-S-CIFAR10.prototxt"
# Input resolution
image_size: 32
# Number of classes
num_classes: 10
downsample_ratio: 0.25
# Prepare hyperparameters for training
optimizer: sgd
# Learning rate schedule
lr_schedule: cosine
# Total batch size and LR on all GPUs.
train_batch_size: 96
val_batch_size: 200
lr: 0.025
epochs: 20
start_epochs: 0
# This is used to set the batchnorm settings.
bn_momentum: 0.01
bn_epsilon: 1e-3
# Config SE options.
se_intra_activation_fn: relu
se_output_activation_fn: sigmoid
# Exponential Moving Average.
EMA: 0.99
use_num_updates: 1
# Default regularization. This is configured for layers without a user-defined L2 regularization strength.
regularizer: 'l2'
weight_decay: 3e-4
# Configure other regularization
drop_connect: 0.0
label_smoothing: 0.0
# Report frequency
report_freq: 50
# Distributed options. Use 0, 1 as boolean variable.
multiprocessing_distributed: 0
# CUDNN deterministic:
cudnn_deterministic: 0
# AMP (Auto mixed precision)
amp: 0
# regularize_depthwise
regularize_depthwise: 1